# dataflow/cli_funcs/cli_eval.py
"""DataFlow è¯„ä¼°å·¥å…· - ä¿®å¤ç‰ˆæœ¬"""

import os
import json
import shutil
import subprocess
import argparse
import importlib.util
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

from dataflow import get_logger
from dataflow.serving import LocalModelLLMServing_vllm
from dataflow.operators.reasoning import ReasoningAnswerGenerator
from dataflow.prompts.reasoning.diy import DiyAnswerGeneratorPrompt
from dataflow.utils.storage import FileStorage
import torch
import gc

logger = get_logger()

# é»˜è®¤çš„ç­”æ¡ˆç”Ÿæˆæç¤ºè¯æ¨¡æ¿
DEFAULT_ANSWER_PROMPT = """Please answer the following question based on the provided academic literature. Your response should:
1. Provide accurate information from the source material
2. Include relevant scientific reasoning and methodology
3. Reference specific findings, data, or conclusions when applicable
4. Maintain academic rigor and precision in your explanation

Question: {question}

Answer:"""


# =============================================================================
# è¯„ä¼°ç®¡é“æ ¸å¿ƒé€»è¾‘
# =============================================================================

class EvaluationPipeline:
    """è¯„ä¼°ç®¡é“ï¼šä»æ¨¡å‹æ£€æµ‹åˆ°è¯„ä¼°æŠ¥å‘Šçš„å®Œæ•´æµç¨‹"""

    def __init__(self, config: Dict[str, Any], cli_args=None):
        self.config = config
        self.cli_args = cli_args or argparse.Namespace()
        self.detected_models = []
        self.prepared_models = []
        self.generated_files = []

    # è¿è¡Œæ­¥éª¤
    def run(self) -> bool:
        try:
            # 1. è·å–ç›®æ ‡æ¨¡å‹
            logger.info("Loading target models...")
            self.target_models = self._get_target_models()  # æ–°æ–¹æ³•
            if not self.target_models:
                self._show_no_models_help()
                return False

            self.prepared_models = self._prepare_models()

            # 2. ç”Ÿæˆç­”æ¡ˆ
            logger.info("Starting answer generation...")
            self.generated_files = self._generate_answers()

            # 3. æ‰§è¡Œè¯„ä¼°
            logger.info("Starting evaluation...")
            results = self._run_evaluation()

            # 4. ç”ŸæˆæŠ¥å‘Š
            self._generate_report(results)

            return True

        except Exception as e:
            logger.error(f"Evaluation pipeline failed: {e}")
            return False

    def _get_target_models(self) -> List[str]:
        """ä»é…ç½®æ–‡ä»¶è·å–ç›®æ ‡æ¨¡å‹åˆ—è¡¨"""
        target_config = self.config.get("TARGET_MODELS", {})
        
        # æ”¯æŒæ ¼å¼ï¼šåˆ—è¡¨æ ¼å¼ è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–æ ¼å¼
        if isinstance(target_config, list):
            models = target_config
        else:
            logger.error(f"Invalid TARGET_MODELS format: {type(target_config)}")
            return []
        
        if not models:
            logger.error("No models specified in TARGET_MODELS configuration")
            return []
            
        logger.info(f"Target models from config: {models}")     
        return models


    def _show_no_models_help(self):
        """æ˜¾ç¤ºæ— æ¨¡å‹æ—¶çš„å¸®åŠ©ä¿¡æ¯"""
        logger.error("No valid models found!")
        print()
        print("=" * 60)
        print("No models available for evaluation")
        print("=" * 60)
        print()
        print("Solutions:")
        print()
        print("1. Edit your evaluation config file:")
        print("   TARGET_MODELS = [")
        print("       'Qwen/Qwen2.5-7B-Instruct',")
        print("       'meta-llama/Llama-3-8B-Instruct',")
        print("       '/path/to/your/local/model'")
        print("   ]")
        print()
        print("2. Or use dictionary format:")
        print("   TARGET_MODELS = {")
        print("       'models': [")
        print("           'Qwen/Qwen2.5-7B-Instruct',")
        print("           '/path/to/local/model'")
        print("       ]")
        print("   }")
        print()
        print("3. Specify models via command line:")
        print("   dataflow eval local --models 'Qwen/Qwen2.5-7B,/path/to/model'")
        print()
        print("=" * 60)


    def _prepare_models(self) -> List[Dict[str, str]]:
        """å‡†å¤‡æ¨¡å‹ä¿¡æ¯"""
        prepared = []
        for model_path in self.target_models:
            model_name = Path(model_path).name
            prepared.append({
                "name": model_name,
                "path": model_path,
                "type": "local"
            })
        return prepared
    def _clear_vllm_cache(self):
        """æ¸…ç† vLLM ç¼–è¯‘ç¼“å­˜"""
        cache_paths = [
            Path.home() / ".cache" / "vllm" / "torch_compile_cache",
            Path.home() / ".cache" / "vllm"
        ]
        
        for cache_path in cache_paths:
            if cache_path.exists():
                try:
                    shutil.rmtree(cache_path)
                    logger.info(f"Cleared vLLM cache: {cache_path}")
                except Exception as e:
                    logger.warning(f"Failed to clear cache {cache_path}: {e}")
    def _generate_answers(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¨¡å‹ç­”æ¡ˆ"""
        import time
        
        generated_files = []
        data_config = self.config.get("DATA_CONFIG", {})
        input_file = data_config.get("input_file", "./.cache/data/qa.json")

        if not Path(input_file).exists():
            logger.error(f"Evaluation data file does not exist: {input_file}")
            return []
        
        self._clear_vllm_cache()  # æ¸…ç†ç£ç›˜ç¼“å­˜

        for idx, model_info in enumerate(self.prepared_models, 1):
            llm_serving = None
            answer_generator = None
            storage = None
            
            try:
                logger.info(f"[{idx}/{len(self.prepared_models)}] Loading: {model_info['name']}")
                
                # ========== åŠ è½½æ¨¡å‹ ==========
                Path("./.cache/eval").mkdir(parents=True, exist_ok=True)
                output_file = f"./.cache/eval/answers_{model_info['name']}.json"
                
                llm_serving = LocalModelLLMServing_vllm(
                    hf_model_name_or_path=model_info['path'],
                    vllm_tensor_parallel_size=2,
                    vllm_max_tokens=1024,
                    vllm_gpu_memory_utilization=0.8
                )
                logger.info(f"Model loaded: {model_info['name']}")
                
                # ========== åˆ›å»ºç­”æ¡ˆç”Ÿæˆå™¨ ==========
                answer_generator = ReasoningAnswerGenerator(
                    llm_serving=llm_serving,
                    prompt_template=DiyAnswerGeneratorPrompt(DEFAULT_ANSWER_PROMPT)
                )
                
                # ========== åˆ›å»ºå­˜å‚¨ ==========
                cache_path = f"./.cache/eval/{model_info['name']}_generation"
                storage = FileStorage(
                    first_entry_file_name=input_file,
                    cache_path=cache_path,
                    file_name_prefix="answer_gen",
                    cache_type="json"
                )
                
                # ========== ç”Ÿæˆç­”æ¡ˆ ==========
                logger.info(f"Generating answers for {model_info['name']}...")
                question_key = data_config.get("question_key", "input")
                
                answer_generator.run(
                    storage=storage.step(),
                    input_key=question_key,
                    output_key="model_generated_answer"
                )
                
                # ========== ä¿å­˜ç»“æœ ==========
                generated_data_path = f"{cache_path}/answer_gen_step1.json"
                if Path(generated_data_path).exists():
                    shutil.copy2(generated_data_path, output_file)
                    logger.info(f"Answers saved to: {output_file}")
                    
                    generated_files.append({
                        "model_name": model_info['name'],
                        "model_path": model_info['path'],
                        "file_path": output_file
                    })
                else:
                    logger.error(f"Generated file not found: {generated_data_path}")
                    continue
                
            except Exception as e:
                logger.error(f"Failed: {e}")
                import traceback
                traceback.print_exc()  # æ‰“å°è¯¦ç»†é”™è¯¯å †æ ˆ
                continue
                
            finally:
                # ========== æ¸…ç†GPUæ˜¾å­˜ ==========
                logger.info(f"Releasing GPU memory for {model_info['name']}...")
                
                if answer_generator is not None:
                    del answer_generator
                if storage is not None:
                    del storage
                if llm_serving is not None:
                    del llm_serving
                
                gc.collect()
                
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                
                logger.info(f"âœ“ Memory released\n")
        
        return generated_files

    def _create_eval_result_path(self, model_name: str) -> str:
        """åˆ›å»ºåŸºäºæ—¶é—´çš„è¯„ä¼°ç»“æœè·¯å¾„"""
        timestamp = datetime.now()
        date_str = timestamp.strftime("%Y-%m-%d")
        time_str = timestamp.strftime("%H-%M-%S")

        # æ¸…ç†æ¨¡å‹åç§°
        clean_model_name = "".join(c for c in model_name if c.isalnum() or c in ('-', '_'))

        # åˆ›å»ºç›®å½•ç»“æ„
        eval_dir = Path("./eval_results") / date_str / f"{time_str}_{clean_model_name}"
        eval_dir.mkdir(parents=True, exist_ok=True)

        return str(eval_dir / "eval_result.json")

    def _save_model_info(self, model_name: str, eval_dir: Path, model_path: str = ""):
        """ä¿å­˜æ¨¡å‹ä¿¡æ¯å’Œé…ç½®"""
        model_info = {
            "model_name": model_name,
            "model_path": model_path,
            "evaluation_time": datetime.now().isoformat(),
            "eval_config": self.config.get("EVAL_CONFIG", {}),
            "judge_model": self.config.get("JUDGE_MODEL_CONFIG", {}).get("model_name", "unknown")
        }

        with open(eval_dir / "model_info.json", 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)

    def _run_evaluation(self) -> List[Dict[str, Any]]:
        """è¿è¡Œè¯„ä¼°"""
        data_config = self.config.get("DATA_CONFIG", {})

        # åˆ›å»ºjudge serving
        try:
            judge_serving = self.config["create_judge_serving"]()
        except Exception as e:
            logger.error(f"Failed to create evaluation service: {e}")
            return []

        results = []

        for file_info in self.generated_files:
            logger.info(f"Evaluating model: {file_info['model_name']}")

            try:
                # åˆ›å»ºåŸºäºæ—¶é—´çš„ç»“æœè·¯å¾„
                result_file = self._create_eval_result_path(file_info['model_name'])
                eval_dir = Path(result_file).parent

                # ä¿å­˜æ¨¡å‹ä¿¡æ¯
                self._save_model_info(file_info['model_name'], eval_dir, file_info.get('model_path', ''))

                # åˆ›å»ºå­˜å‚¨ï¼ˆä½¿ç”¨.cache/evalç›®å½•ï¼‰
                cache_name = "".join(c for c in file_info['model_name'] if c.isalnum() or c in ('-', '_'))
                storage = self.config["create_storage"](
                    file_info["file_path"],
                    f"./.cache/eval/{cache_name}"
                )

                # åˆ›å»ºè¯„ä¼°å™¨
                evaluator = self.config["create_evaluator"](judge_serving, result_file)

                # è¿è¡Œè¯„ä¼°
                evaluator_config = self.config.get("EVALUATOR_RUN_CONFIG", {})
                evaluator.run(
                    storage=storage.step(),
                    input_test_answer_key=evaluator_config.get("input_test_answer_key", "model_generated_answer"),
                    input_gt_answer_key=evaluator_config.get("input_gt_answer_key", "output"),
                    input_question_key=evaluator_config.get("input_question_key", "input")
                )

                # è¯»å–ç»“æœ
                if Path(result_file).exists():
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result_data = json.load(f)
                        if isinstance(result_data, list) and result_data:
                            result_data[0]["model_name"] = file_info['model_name']
                            result_data[0]["result_file"] = result_file
                            results.append(result_data[0])

            except Exception as e:
                logger.error(f"Failed to evaluate model {file_info['model_name']}: {e}")
                continue

        return results

    def _generate_report(self, results: List[Dict[str, Any]]):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        if not results:
            logger.warning("No valid evaluation results")
            return

        print("\n" + "=" * 60)
        print("Model Evaluation Results Comparison")
        print("=" * 60)

        # æŒ‰å‡†ç¡®ç‡æ’åº
        sorted_results = sorted(results, key=lambda x: x.get("accuracy", 0), reverse=True)

        for i, result in enumerate(sorted_results, 1):
            print(f"{i}. {result.get('model_name', 'Unknown')}")
            print(f"   Accuracy: {result.get('accuracy', 0):.3f}")
            print(f"   Total samples: {result.get('total_samples', 0)}")
            print(f"   Matched samples: {result.get('matched_samples', 0)}")
            print(f"   Result file: {result.get('result_file', 'N/A')}")
            print()

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report = {
            "evaluation_summary": {
                "total_models": len(results),
                "evaluation_time": datetime.now().isoformat(),
                "results": sorted_results
            }
        }

        report_file = "./eval_results/model_comparison_report.json"
        Path(report_file).parent.mkdir(parents=True, exist_ok=True)
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"Detailed report saved: {report_file}")
        print("=" * 60)

# =============================================================================
# CLIå·¥å…·ç±»
# =============================================================================

class DataFlowEvalCLI:
    """DataFlow è¯„ä¼°å‘½ä»¤è¡Œå·¥å…·"""

    def __init__(self):
        self.current_dir = Path.cwd()

    def _get_template_path(self, eval_type: str) -> Path:
        """è·å–æ¨¡æ¿æ–‡ä»¶è·¯å¾„"""
        # ç›´æ¥ç¡¬ç¼–ç è·¯å¾„
        current_file = Path(__file__)  # cli_eval.pyçš„è·¯å¾„
        dataflow_dir = current_file.parent.parent  # dataflowç›®å½•
        template_path = dataflow_dir / "cli_funcs" / "eval_pipeline" / f"eval_{eval_type}.py"
        return template_path

    def init_eval_files(self):
        """ç”Ÿæˆè¯„ä¼°é…ç½®æ–‡ä»¶ - ä¸€æ¬¡æ€§å¤åˆ¶ä¸¤ä¸ªæ¨¡æ¿"""
        files_to_create = [
            ("eval_api.py", "api"),
            ("eval_local.py", "local")
        ]

        created_files = []
        existing_files = []

        # æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡ä»¶
        for filename, eval_type in files_to_create:
            output_path = self.current_dir / filename
            if output_path.exists():
                existing_files.append(filename)

        # å¦‚æœæœ‰æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯¢é—®æ˜¯å¦è¦†ç›–
        if existing_files:
            logger.warning(f"Following files already exist: {', '.join(existing_files)}")
            user_input = input("Overwrite? (y/n): ").strip().lower()
            if user_input != 'y':
                logger.info("Operation cancelled")
                return False

        # å¤åˆ¶æ–‡ä»¶
        for filename, eval_type in files_to_create:
            try:
                output_path = self.current_dir / filename

                # è·å–å†…ç½®æ¨¡æ¿æ–‡ä»¶è·¯å¾„
                template_path = self._get_template_path(eval_type)

                if not template_path.exists():
                    logger.error(f"Built-in template file does not exist: {template_path}")
                    continue

                # å¤åˆ¶æ–‡ä»¶
                shutil.copy2(template_path, output_path)
                created_files.append(filename)
                logger.info(f"Created: {filename}")

            except Exception as e:
                logger.error(f"Failed to create {filename}: {e}")
                continue

        if created_files:
            logger.info(f"\nEvaluation config files initialization complete!")
            logger.info("Created files:")
            for filename in created_files:
                logger.info(f"  - {filename}")
            logger.info("\nUsage:")
            logger.info("  dataflow eval api    # API model evaluation")
            logger.info("  dataflow eval local  # Local model evaluation")
            logger.info("\nYou must edit these files to customize evaluation configuration")
            return True
        else:
            logger.error("No config files created successfully")
            return False

    def _validate_config(self, config: Dict[str, Any], eval_type: str = None) -> bool:
        """éªŒè¯é…ç½®æ–‡ä»¶çš„å¿…è¦å‚æ•° - æ ¹æ®è¯„ä¼°ç±»å‹åŒºåˆ†éªŒè¯"""
        required_keys = [
            "TARGET_MODELS",
            "JUDGE_MODEL_CONFIG",
            "DATA_CONFIG",
            "create_judge_serving",
            "create_evaluator",
            "create_storage"
        ]

        for key in required_keys:
            if key not in config:
                logger.error(f"Missing required config key: {key}")
                return False

        
        # éªŒè¯ TARGET_MODELS ç»“æ„ - æ”¯æŒä¸¤ç§æ ¼å¼
        target_models = config.get("TARGET_MODELS", {})
        if isinstance(target_models, list):
            # æ–°æ ¼å¼ï¼šç›´æ¥åˆ—è¡¨
            if not target_models:
                logger.warning("TARGET_MODELS list is empty")
        elif isinstance(target_models, dict):
            # åŸæ ¼å¼ï¼šå­—å…¸åŒ…å«models
            logger.debug("TARGET_MODELS is dict format")
            if "models" not in target_models:
                logger.error("TARGET_MODELS dict missing 'models' key")
                return False
        else:
            logger.error(f"TARGET_MODELS must be a list or dictionary, got {type(target_models)}")
            return False

        # éªŒè¯ JUDGE_MODEL_CONFIG - æ ¹æ®è¯„ä¼°ç±»å‹åŒºåˆ†
        judge_config = config.get("JUDGE_MODEL_CONFIG", {})
        if not isinstance(judge_config, dict):
            logger.error("JUDGE_MODEL_CONFIG must be a dictionary")
            return False
        logger.debug("âœ… Configuration validation passed")
        return True

    def run_eval_file(self, eval_type: str, eval_file: str, cli_args):
        """è¿è¡Œè¯„ä¼°"""
        eval_path = self.current_dir / eval_file

        if not eval_path.exists():
            logger.error(f"Evaluation config file does not exist: {eval_path}")
            logger.info(f"Please run first: dataflow eval init")
            return False

        try:
            # åŠ¨æ€å¯¼å…¥ç”¨æˆ·çš„é…ç½®æ–‡ä»¶
            config = self._import_user_config(eval_path)
            if not config:
                logger.error("âŒ Configuration file parameters are incorrect")
                logger.info("ğŸ’¡ Please check your config file contains get_evaluator_config() function")
                return False

            # éªŒè¯é…ç½®æ–‡ä»¶å¿…è¦å‚æ•° - ä¿®å¤ï¼šç¡®ä¿ä¼ å…¥æ­£ç¡®çš„è¯„ä¼°ç±»å‹
            logger.debug(f"Validating config for eval_type: {eval_type}")
            if not self._validate_config(config, eval_type):
                logger.error("âŒ Configuration file parameters are incorrect")
                logger.info("ğŸ’¡ Please run 'dataflow eval init' to regenerate config files")
                return False

            # åº”ç”¨CLIå‚æ•°è¦†ç›–
            if hasattr(cli_args, 'models') and cli_args.models:
                model_list = [m.strip() for m in cli_args.models.split(',')]
                config["TARGET_MODELS"]["models"] = model_list
                logger.info(f"Using CLI specified models: {model_list}")

            # è¿è¡Œè¯„ä¼°
            success = run_evaluation(config, cli_args)
            return success

        except Exception as e:
            logger.error(f"âŒ Configuration file parameters are incorrect: {e}")
            logger.info("ğŸ’¡ Troubleshooting steps:")
            logger.info(f"   1. Check config file syntax: python -m py_compile {eval_file}")
            logger.info("   2. Regenerate config: dataflow eval init")
            if eval_type == "api":
                logger.info("   3. Verify API keys: echo $DF_API_KEY")
            else:
                logger.info("   3. Check local model paths in config")
            return False

    def _import_user_config(self, config_path: Path):
        """åŠ¨æ€å¯¼å…¥ç”¨æˆ·é…ç½®æ–‡ä»¶"""
        try:
            # åˆ›å»ºæ¨¡å—è§„èŒƒ
            spec = importlib.util.spec_from_file_location("user_eval_config", config_path)
            if spec is None:
                logger.error(f"Cannot create module spec: {config_path}")
                return None

            # åˆ›å»ºæ¨¡å—
            user_config_module = importlib.util.module_from_spec(spec)

            # æ‰§è¡Œæ¨¡å—
            spec.loader.exec_module(user_config_module)

            # è·å–é…ç½®
            if hasattr(user_config_module, 'get_evaluator_config'):
                config = user_config_module.get_evaluator_config()
                logger.info(f"Successfully loaded config: {config_path}")
                return config
            else:
                logger.error(f"Config file missing get_evaluator_config() function: {config_path}")
                return None

        except Exception as e:
            logger.error(f"Failed to import config file: {e}")
            logger.error(f"Please check config file syntax: {config_path}")
            return None
# =============================================================================
# ç®€åŒ–çš„æ‰§è¡Œæ¥å£
# =============================================================================

def run_evaluation(config, cli_args=None):
    """è¿è¡Œè¯„ä¼°"""
    logger.info("DataFlow model evaluation started")

    try:
        pipeline = EvaluationPipeline(config, cli_args)
        success = pipeline.run()

        if success:
            logger.info("Model evaluation completed")
        else:
            logger.error("Model evaluation failed")

        return success

    except Exception as e:
        logger.error(f"Evaluation failed: {str(e)}")
        raise