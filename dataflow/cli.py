#!/usr/bin/env python3
# dataflow/cli.py - Enhanced with local model judge support and eval init/run
# ===============================================================
# DataFlow å‘½ä»¤è¡Œå…¥å£
#   dataflow -v                         æŸ¥çœ‹ç‰ˆæœ¬å¹¶æ£€æŸ¥æ›´æ–°
#   dataflow init [...]                åˆå§‹åŒ–è„šæœ¬/é…ç½®
#   dataflow env                       æŸ¥çœ‹ç¯å¢ƒ
#   dataflow webui operators [opts]    å¯åŠ¨ç®—å­/ç®¡çº¿ UI
#   dataflow webui agent     [opts]    å¯åŠ¨ DataFlow-Agent UIï¼ˆå·²æ•´åˆåç«¯ï¼‰
#   dataflow pdf2model init/train      PDF to Model è®­ç»ƒæµç¨‹
#   dataflow text2model init/train     Text to Model è®­ç»ƒæµç¨‹
#   dataflow chat                      èŠå¤©ç•Œé¢
#   dataflow eval init                 åˆå§‹åŒ–è¯„ä¼°é…ç½®æ–‡ä»¶
#   dataflow eval api                  è¿è¡ŒAPIæ¨¡å‹è¯„ä¼°
#   dataflow eval local                è¿è¡Œæœ¬åœ°æ¨¡å‹è¯„ä¼°
#   dataflow eval list                 åˆ—å‡ºè¯„ä¼°é…ç½®æ–‡ä»¶
# ===============================================================

import os
import argparse
import requests
import sys
import re
import yaml
import json
import subprocess
from pathlib import Path
from colorama import init as color_init, Fore, Style
from dataflow.cli_funcs import cli_env, cli_init  # é¡¹ç›®å·²æœ‰å·¥å…·
from dataflow.version import __version__  # ç‰ˆæœ¬å·

color_init(autoreset=True)
PYPI_API_URL = "https://pypi.org/pypi/open-dataflow/json"


# ---------------- ç‰ˆæœ¬æ£€æŸ¥ ----------------
def version_and_check_for_updates() -> None:
    width = os.get_terminal_size().columns
    print(Fore.BLUE + "=" * width + Style.RESET_ALL)
    print(f"open-dataflow codebase version: {__version__}")

    try:
        r = requests.get(PYPI_API_URL, timeout=5)
        r.raise_for_status()
        remote = r.json()["info"]["version"]
        print("\tChecking for updates...")
        print(f"\tLocal version : {__version__}")
        print(f"\tPyPI  version : {remote}")
        if remote != __version__:
            print(Fore.YELLOW + f"New version available: {remote}."
                                "  Run 'pip install -U open-dataflow' to upgrade."
                  + Style.RESET_ALL)
        else:
            print(Fore.GREEN + f"You are using the latest version: {__version__}" + Style.RESET_ALL)
    except requests.exceptions.RequestException as e:
        print(Fore.RED + "Failed to query PyPI â€“ check your network." + Style.RESET_ALL)
        print("Error:", e)
    print(Fore.BLUE + "=" * width + Style.RESET_ALL)


# ---------------- æ™ºèƒ½èŠå¤©åŠŸèƒ½ ----------------
def check_current_dir_for_model():
    """æ£€æŸ¥å½“å‰ç›®å½•çš„æ¨¡å‹æ–‡ä»¶ï¼Œä¼˜å…ˆè¯†åˆ«å¾®è°ƒæ¨¡å‹"""
    current_dir = Path.cwd()

    # æ£€æŸ¥ LoRA é€‚é…å™¨æ–‡ä»¶
    adapter_files = [
        "adapter_config.json",
        "adapter_model.bin",
        "adapter_model.safetensors"
    ]

    # æ£€æŸ¥åŸºç¡€æ¨¡å‹æ–‡ä»¶
    model_files = [
        "config.json",
        "pytorch_model.bin",
        "model.safetensors",
        "tokenizer.json",
        "tokenizer_config.json"
    ]

    # ä¼˜å…ˆæ£€æŸ¥adapterï¼ˆå¾®è°ƒæ¨¡å‹ï¼‰
    # å¦‚æœæœ‰adapteræ–‡ä»¶ï¼Œå°±åªè¿”å›å¾®è°ƒæ¨¡å‹ï¼Œä¸ç®¡æœ‰æ²¡æœ‰åŸºç¡€æ¨¡å‹æ–‡ä»¶
    if any((current_dir / f).exists() for f in adapter_files):
        return [("fine_tuned_model", current_dir)]

    # åªæœ‰åœ¨æ²¡æœ‰adapteræ–‡ä»¶æ—¶ï¼Œæ‰æ£€æŸ¥base model
    if any((current_dir / f).exists() for f in model_files):
        return [("base_model", current_dir)]

    return []


def get_latest_trained_model(cache_path="./"):
    """æŸ¥æ‰¾æœ€æ–°è®­ç»ƒçš„æ¨¡å‹ï¼Œæ”¯æŒtext2modelå’Œpdf2modelï¼ŒæŒ‰æ—¶é—´æˆ³æ’åº"""
    current_dir = Path.cwd()
    cache_path_obj = Path(cache_path)
    if not cache_path_obj.is_absolute():
        cache_path_obj = current_dir / cache_path_obj

    saves_dir = cache_path_obj / ".cache" / "saves"
    if not saves_dir.exists():
        return None, None

    all_models = []

    for dir_path in saves_dir.iterdir():
        if not dir_path.is_dir():
            continue

        model_type = None
        timestamp = None

        # æ£€æŸ¥text2modelæ ¼å¼ (text2model_cache_YYYYMMDD_HHMMSS)
        if dir_path.name.startswith('text2model_cache_'):
            timestamp_part = dir_path.name.replace('text2model_cache_', '')
            if len(timestamp_part) == 15 and timestamp_part[8] == '_':
                date_part = timestamp_part[:8]
                time_part = timestamp_part[9:]
                if date_part.isdigit() and time_part.isdigit() and len(time_part) == 6:
                    model_type = 'text2model'
                    timestamp = timestamp_part

        # æ£€æŸ¥pdf2modelæ ¼å¼ (pdf2model_cache_YYYYMMDD_HHMMSS)
        elif dir_path.name.startswith('pdf2model_cache_'):
            timestamp_part = dir_path.name.replace('pdf2model_cache_', '')
            if len(timestamp_part) == 15 and timestamp_part[8] == '_':
                date_part = timestamp_part[:8]
                time_part = timestamp_part[9:]
                if date_part.isdigit() and time_part.isdigit() and len(time_part) == 6:
                    model_type = 'pdf2model'
                    timestamp = timestamp_part

        # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„æ¨¡å‹ç›®å½•
        else:
            # å°è¯•ä»ç›®å½•åæå–æ—¶é—´æˆ³
            timestamp_match = re.search(r'(\d{8}_\d{6})', dir_path.name)
            if timestamp_match:
                model_type = 'pdf2model'  # é»˜è®¤ä¸ºpdf2model
                timestamp = timestamp_match.group(1)
            elif 'qwen' in dir_path.name.lower() or 'model' in dir_path.name.lower():
                # å¦‚æœæ‰¾ä¸åˆ°æ—¶é—´æˆ³ä½†çœ‹èµ·æ¥åƒæ¨¡å‹ç›®å½•ï¼Œä½¿ç”¨ä¿®æ”¹æ—¶é—´
                model_type = 'pdf2model'  # é»˜è®¤ä¸ºpdf2model
                mtime = dir_path.stat().st_mtime
                # å°†ä¿®æ”¹æ—¶é—´è½¬æ¢ä¸ºtimestampæ ¼å¼ä»¥ä¾¿æ’åº
                import datetime
                dt = datetime.datetime.fromtimestamp(mtime)
                timestamp = dt.strftime("%Y%m%d_%H%M%S")

        if model_type and timestamp:
            all_models.append((dir_path, model_type, timestamp))

    if not all_models:
        return None, None

    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œæœ€æ–°çš„åœ¨å‰ï¼ˆä¸ç®¡æ˜¯ä»€ä¹ˆç±»å‹çš„æ¨¡å‹ï¼‰
    all_models.sort(key=lambda x: x[2], reverse=True)
    latest_model_path, model_type, timestamp = all_models[0]

    return latest_model_path, model_type


def call_dataflow_chat(model_path, model_type=None):
    """è°ƒç”¨dataflowçš„èŠå¤©åŠŸèƒ½ï¼ˆç”¨äºå¾®è°ƒæ¨¡å‹ï¼‰"""
    # åˆ¤æ–­æ¨¡å‹ç±»å‹
    if model_type is None:
        # ä»è·¯å¾„åˆ¤æ–­ç±»å‹
        path_str = str(model_path)
        if 'text2model' in path_str:
            model_type = 'text2model'
        elif 'pdf2model' in path_str:
            model_type = 'pdf2model'
        else:
            # æ— æ³•åˆ¤æ–­ï¼Œé»˜è®¤å°è¯•text2model
            model_type = 'text2model'

    if model_type == 'text2model':
        try:
            from dataflow.cli_funcs.cli_text import cli_text2model_chat
            return cli_text2model_chat(str(model_path))
        except ImportError:
            print("Cannot find text model chat function")
            return False
    elif model_type == 'pdf2model':
        try:
            from dataflow.cli_funcs.cli_pdf import cli_pdf2model_chat
            return cli_pdf2model_chat(str(model_path))
        except ImportError:
            print("Cannot find PDF model chat function")
            return False
    else:
        print(f"Unknown model type: {model_type}")
        return False


def call_llamafactory_chat(model_path):
    """è°ƒç”¨llamafactoryçš„èŠå¤©åŠŸèƒ½ï¼ˆç”¨äºåŸºç¡€æ¨¡å‹ï¼‰"""
    import subprocess

    chat_cmd = [
        "llamafactory-cli", "chat",
        "--model_name_or_path", str(model_path)
    ]

    try:
        result = subprocess.run(chat_cmd, check=True)
        return True
    except subprocess.CalledProcessError as e:
        print(f"LlamaFactory chat failed: {e}")
        return False
    except FileNotFoundError:
        print("llamafactory-cli not found. Please install LlamaFactory:")
        print("pip install llamafactory[torch,metrics]")
        return False


def smart_chat_command(model_path=None, cache_path="./"):
    """æ™ºèƒ½èŠå¤©å‘½ä»¤ï¼Œç»Ÿä¸€å¤„ç†å„ç§æ¨¡å‹ç±»å‹ï¼Œä¸è‡ªåŠ¨ä¸‹è½½"""

    if model_path:
        # å¦‚æœæ˜ç¡®æŒ‡å®šäº†æ¨¡å‹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
        model_path_obj = Path(model_path)
        if not model_path_obj.exists():
            print(f"Specified model path does not exist: {model_path}")
            return False

        print(f"{Fore.CYAN}Using specified model: {model_path}{Style.RESET_ALL}")

        # æ£€æŸ¥æ˜¯å¦æœ‰adapteræ–‡ä»¶
        adapter_files = [
            "adapter_config.json",
            "adapter_model.bin",
            "adapter_model.safetensors"
        ]

        has_adapter = any((model_path_obj / f).exists() for f in adapter_files)

        if has_adapter:
            # æœ‰adapterï¼Œä½¿ç”¨dataflow chat
            return call_dataflow_chat(model_path)
        else:
            # æ²¡æœ‰adapterï¼Œä½¿ç”¨llamafactory chat
            return call_llamafactory_chat(model_path)

    # æ£€æŸ¥å½“å‰ç›®å½•
    detected_models = check_current_dir_for_model()

    if detected_models:
        # ä¼˜å…ˆä½¿ç”¨fine_tuned_modelï¼ˆadapterï¼‰
        for model_type, path in detected_models:
            if model_type == "fine_tuned_model":
                print(f"{Fore.GREEN}Found trained model in current directory: {path.name}{Style.RESET_ALL}")
                print(f"{Fore.CYAN}Starting chat interface...{Style.RESET_ALL}")
                return call_dataflow_chat(path)

        # å¦‚æœæ²¡æœ‰adapterï¼Œä½¿ç”¨base_model
        for model_type, path in detected_models:
            if model_type == "base_model":
                print(f"{Fore.YELLOW}Found base model in current directory: {path.name}{Style.RESET_ALL}")
                print(f"{Fore.CYAN}Starting chat interface...{Style.RESET_ALL}")
                return call_llamafactory_chat(path)

    # æ£€æŸ¥ç¼“å­˜ä¸­çš„è®­ç»ƒæ¨¡å‹
    latest_model, model_type = get_latest_trained_model(cache_path)

    if latest_model:
        model_name = Path(latest_model).name
        print(f"{Fore.GREEN}Found trained model from cache: {model_name}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}Starting chat interface...{Style.RESET_ALL}")

        # æ£€æŸ¥ç¼“å­˜ä¸­çš„æ¨¡å‹æ˜¯å¦æœ‰adapteræ–‡ä»¶
        latest_model_path = Path(latest_model)
        adapter_files = [
            "adapter_config.json",
            "adapter_model.bin",
            "adapter_model.safetensors"
        ]

        has_adapter = any((latest_model_path / f).exists() for f in adapter_files)
        if has_adapter:
            return call_dataflow_chat(latest_model, model_type)
        else:
            print(f"No adapter files found in {latest_model}")
            print("This doesn't appear to be a trained model directory.")
            return False

    # å¦‚æœä»€ä¹ˆéƒ½æ²¡æ‰¾åˆ°ï¼Œç»™å‡ºæç¤ºè€Œä¸ä¸‹è½½
    print("No model found in current directory or cache.")
    print()
    print("Options:")
    print("1. Train a model first:")
    print("   dataflow text2model init && dataflow text2model train")
    print("   dataflow pdf2model init && dataflow pdf2model train")
    print()
    print("2. Use an existing model:")
    print("   dataflow chat --model /path/to/your/model")
    print()
    print("3. Download a model manually and place it in current directory")
    return False


# ---------------- æ–°çš„evalå‘½ä»¤å¤„ç†å‡½æ•° ----------------
def handle_python_config_init(eval_type: str, output_file: str = None):
    """å¤„ç†Pythoné…ç½®æ–‡ä»¶åˆå§‹åŒ–"""
    try:
        from dataflow.cli_funcs.cli_eval import DataFlowEvalCLI
        
        cli = DataFlowEvalCLI()
        success = cli.init_eval_file(eval_type, output_file)
        
        if success:
            print("âœ… é…ç½®æ–‡ä»¶åˆå§‹åŒ–æˆåŠŸ")
        else:
            print("âŒ é…ç½®æ–‡ä»¶åˆå§‹åŒ–å¤±è´¥")
            
        return success
        
    except ImportError as e:
        print(f"Pythoné…ç½®è¯„ä¼°æ¨¡å—ä¸å¯ç”¨ï¼š{e}")
        print("è¯·æ£€æŸ¥ dataflow.cli_funcs.cli_eval æ¨¡å—æ˜¯å¦å­˜åœ¨")
        return False
    except Exception as e:
        print(f"é…ç½®æ–‡ä»¶åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        return False


def handle_python_config_eval(eval_type: str, args=None):
    """å¤„ç†Pythoné…ç½®æ–‡ä»¶è¯„ä¼°æ¨¡å¼"""
    try:
        from dataflow.cli_funcs.cli_eval import DataFlowEvalCLI
        
        cli = DataFlowEvalCLI()
        
        # ä½¿ç”¨é»˜è®¤æ–‡ä»¶å
        eval_file = f"eval_{eval_type}.py"
        
        print(f"ğŸš€ å¼€å§‹{eval_type}æ¨¡å‹è¯„ä¼°ï¼š{eval_file}")
        
        # ä¼ é€’å‘½ä»¤è¡Œå‚æ•°åˆ°è¯„ä¼°å™¨
        success = cli.run_eval_file(eval_type, eval_file, args)
        
        if success:
            print(f"âœ… {eval_type}æ¨¡å‹è¯„ä¼°å®Œæˆ")
        else:
            print(f"âŒ {eval_type}æ¨¡å‹è¯„ä¼°å¤±è´¥")
            
        return success
        
    except ImportError as e:
        print(f"Pythoné…ç½®è¯„ä¼°æ¨¡å—ä¸å¯ç”¨ï¼š{e}")
        print("è¯·æ£€æŸ¥ dataflow.cli_funcs.cli_eval æ¨¡å—æ˜¯å¦å­˜åœ¨")
        return False
    except Exception as e:
        print(f"Pythoné…ç½®è¯„ä¼°å¤±è´¥ï¼š{e}")
        return False


def list_eval_files():
    """åˆ—å‡ºè¯„ä¼°é…ç½®æ–‡ä»¶"""
    try:
        from dataflow.cli_funcs.cli_eval import DataFlowEvalCLI
        
        cli = DataFlowEvalCLI()
        cli.list_eval_files()
        return True
        
    except ImportError:
        print("Pythoné…ç½®è¯„ä¼°æ¨¡å—ä¸å¯ç”¨")
        return False
    except Exception as e:
        print(f"åˆ—å‡ºé…ç½®æ–‡ä»¶å¤±è´¥ï¼š{e}")
        return False


def handle_eval_command(args):
    """Handle evaluation command - æ”¯æŒè‡ªåŠ¨æ£€æµ‹å’Œæ¨¡å‹æŒ‡å®š"""
    try:
        eval_action = getattr(args, 'eval_action', None)
        
        # å¤„ç† init å­å‘½ä»¤
        if eval_action == 'init':
            return handle_python_config_init(args.type, args.output)
        
        # å¤„ç† api å­å‘½ä»¤ï¼ˆå¢å¼ºç‰ˆï¼‰
        elif eval_action == 'api':
            return handle_python_config_eval('api', args)
        
        # å¤„ç† local å­å‘½ä»¤ï¼ˆå¢å¼ºç‰ˆï¼‰
        elif eval_action == 'local':
            return handle_python_config_eval('local', args)
        
        # å¤„ç† list å­å‘½ä»¤
        elif eval_action == 'list':
            return list_eval_files()
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå­å‘½ä»¤ï¼Œæ˜¾ç¤ºå¸®åŠ©
        else:
            print("DataFlow è¯„ä¼°å·¥å…·")
            print()
            print("å¯ç”¨å‘½ä»¤:")
            print("  dataflow eval init [--type api/local]     # åˆå§‹åŒ–è¯„ä¼°é…ç½®æ–‡ä»¶")
            print("  dataflow eval api                         # è¿è¡ŒAPIæ¨¡å‹è¯„ä¼°ï¼ˆè‡ªåŠ¨æ£€æµ‹æ¨¡å‹ï¼‰")
            print("  dataflow eval local                       # è¿è¡Œæœ¬åœ°æ¨¡å‹è¯„ä¼°ï¼ˆè‡ªåŠ¨æ£€æµ‹æ¨¡å‹ï¼‰")
            print("  dataflow eval list                        # åˆ—å‡ºé…ç½®æ–‡ä»¶")
            print()
            print("é«˜çº§ç”¨æ³•:")
            print("  dataflow eval api --models model1,model2  # æŒ‡å®šç‰¹å®šæ¨¡å‹è¿›è¡Œè¯„ä¼°")
            print("  dataflow eval api --no-auto               # ç¦ç”¨è‡ªåŠ¨æ£€æµ‹ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹")
            print()
            print("å®Œæ•´è¯„ä¼°æµç¨‹:")
            print("  1. dataflow eval api                      # è‡ªåŠ¨æ£€æµ‹æœ¬åœ°æ¨¡å‹å¹¶è¯„ä¼°")
            print("  2. æŸ¥çœ‹ç”Ÿæˆçš„è¯„ä¼°æŠ¥å‘Š                      # model_comparison_report.json")
            print()
            print("é…ç½®æ–‡ä»¶è¯´æ˜:")
            print("  - eval_api.py: APIè¯„ä¼°å™¨é…ç½®ï¼ˆGPT-4oç­‰ï¼‰")
            print("  - eval_local.py: æœ¬åœ°è¯„ä¼°å™¨é…ç½®")
            return False
        
    except Exception as e:
        print(f"è¯„ä¼°å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


# ---------------- CLI ä¸»å‡½æ•° ----------------
def build_arg_parser() -> argparse.ArgumentParser:
    """æ„å»ºå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        prog="dataflow",
        description=f"DataFlow Command-Line Interface  (v{__version__})",
    )
    parser.add_argument("-v", "--version", action="store_true", help="Show version and exit")

    # ============ é¡¶å±‚å­å‘½ä»¤ ============ #
    top = parser.add_subparsers(dest="command", required=False)

    # --- init ---
    p_init = top.add_parser("init", help="Initialize scripts/configs in current dir")
    p_init_sub = p_init.add_subparsers(dest="subcommand", required=False)
    p_init_sub.add_parser("all", help="Init all components").set_defaults(subcommand="all")
    p_init_sub.add_parser("reasoning", help="Init reasoning components").set_defaults(subcommand="reasoning")

    # --- env ---
    top.add_parser("env", help="Show environment information")

    # --- chat ---
    p_chat = top.add_parser("chat", help="Start chat interface with trained model")
    p_chat.add_argument("--model", default=None, help="Model path (default: use latest trained model from cache)")
    p_chat.add_argument("--cache", default="./", help="Cache directory path")

    # --- eval å‘½ä»¤ï¼ˆä¿®æ”¹ç‰ˆæœ¬ï¼Œæ”¯æŒæ¨¡å‹å‚æ•°ï¼‰ ---
    p_eval = top.add_parser("eval", help="Model evaluation using BenchDatasetEvaluator")
    eval_sub = p_eval.add_subparsers(dest="eval_action", help="Evaluation actions")

    # eval init å­å‘½ä»¤
    eval_init = eval_sub.add_parser("init", help="Initialize evaluation configuration file")
    eval_init.add_argument("--type", choices=["api", "local"], default="api",
                          help="Configuration type: api (API models) or local (local models)")
    eval_init.add_argument("--output", help="Output file name (default: eval_api.py or eval_local.py)")

    # eval api å­å‘½ä»¤ï¼ˆå¢å¼ºç‰ˆï¼‰
    eval_api = eval_sub.add_parser("api", help="Run API model evaluation")
    eval_api.add_argument("--models", help="Comma-separated list of models to evaluate (overrides config)")
    eval_api.add_argument("--no-auto", action="store_true", help="Disable auto-detection of models")

    # eval local å­å‘½ä»¤ï¼ˆå¢å¼ºç‰ˆï¼‰
    eval_local = eval_sub.add_parser("local", help="Run local model evaluation")
    eval_local.add_argument("--models", help="Comma-separated list of models to evaluate (overrides config)")
    eval_local.add_argument("--no-auto", action="store_true", help="Disable auto-detection of models")

    # eval list å­å‘½ä»¤
    eval_list = eval_sub.add_parser("list", help="List evaluation configuration files")

    # --- pdf2model ---
    p_pdf2model = top.add_parser("pdf2model", help="PDF to model training pipeline")
    p_pdf2model.add_argument("--cache", default="./", help="Cache directory path")
    p_pdf2model_sub = p_pdf2model.add_subparsers(dest="pdf2model_action", required=True)

    p_pdf2model_init = p_pdf2model_sub.add_parser("init", help="Initialize PDF to model pipeline")

    p_pdf2model_train = p_pdf2model_sub.add_parser("train", help="Start training after PDF processing")
    p_pdf2model_train.add_argument("--lf_yaml", default=None,
                                   help="LlamaFactory config file (default: {cache}/.cache/train_config.yaml)")

    # --- text2model ---
    p_text2model = top.add_parser("text2model", help="Train model from JSON/JSONL data")
    p_text2model_sub = p_text2model.add_subparsers(dest="text2model_action", required=True)

    p_text2model_init = p_text2model_sub.add_parser("init", help="Initialize text2model pipeline")
    p_text2model_init.add_argument("--cache", default="./", help="Cache directory path")

    p_text2model_train = p_text2model_sub.add_parser("train", help="Start training after text processing")
    p_text2model_train.add_argument('input_dir', nargs='?', default='./',
                                    help='Input directory to scan (default: ./)')
    p_text2model_train.add_argument('--input-keys', default=None,
                                    help='Fields to process (default: text)')
    p_text2model_train.add_argument("--lf_yaml", default=None,
                                    help="LlamaFactory config file (default: {cache}/.cache/train_config.yaml)")

    # --- webui ---
    p_webui = top.add_parser("webui", help="Launch Gradio WebUI")
    p_webui.add_argument("-H", "--host", default="127.0.0.1", help="Bind host (default 127.0.0.1)")
    p_webui.add_argument("-P", "--port", type=int, default=7862, help="Port (default 7862)")
    p_webui.add_argument("--show-error", action="store_true", help="Show Gradio error tracebacks")

    #    webui äºŒçº§å­å‘½ä»¤ï¼šoperators / agent
    w_sub = p_webui.add_subparsers(dest="ui_mode", required=False)
    w_sub.add_parser("operators", help="Launch operator / pipeline UI")
    w_sub.add_parser("agent", help="Launch DataFlow-Agent UI (backend included)")
    w_sub.add_parser("pdf", help="Launch PDF Knowledge Base Cleaning UI")

    return parser


def main() -> None:
    """ä¸»å…¥å£å‡½æ•°"""
    parser = build_arg_parser()
    args = parser.parse_args()

    # ---------- é¡¶å±‚é€»è¾‘åˆ†å‘ ----------
    if args.version:
        version_and_check_for_updates()
        return

    if args.command == "init":
        cli_init(subcommand=args.subcommand or "base")

    elif args.command == "env":
        cli_env()

    elif args.command == "eval":
        handle_eval_command(args)

    elif args.command == "pdf2model":
        if args.pdf2model_action == "init":
            from dataflow.cli_funcs.cli_pdf import cli_pdf2model_init
            cli_pdf2model_init(cache_path=args.cache)
        elif args.pdf2model_action == "train":
            from dataflow.cli_funcs.cli_pdf import cli_pdf2model_train
            # If no lf_yaml specified, use default path relative to cache
            lf_yaml = args.lf_yaml or f"{args.cache}/.cache/train_config.yaml"
            cli_pdf2model_train(lf_yaml=lf_yaml, cache_path=args.cache)

    elif args.command == "text2model":
        from dataflow.cli_funcs.cli_text import cli_text2model_init, cli_text2model_train

        if args.text2model_action == "init":
            cli_text2model_init(cache_path=getattr(args, 'cache', './'))
        elif args.text2model_action == "train":
            # å¦‚æœæ²¡æœ‰æŒ‡å®šlf_yamlï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
            lf_yaml = getattr(args, 'lf_yaml', None) or "./.cache/train_config.yaml"
            cli_text2model_train(input_keys=getattr(args, 'input_keys', None), lf_yaml=lf_yaml)

    elif args.command == "chat":
        smart_chat_command(model_path=args.model, cache_path=args.cache)

    elif args.command == "webui":
        # é»˜è®¤ä½¿ç”¨ operators
        mode = args.ui_mode or "operators"
        if mode == "operators":
            from dataflow.webui.operator_pipeline import demo
            demo.launch(
                server_name=args.host,
                server_port=args.port,
                show_error=args.show_error,
            )
        elif mode == "agent":
            from dataflow.agent.webui import app
            import uvicorn
            uvicorn.run(app, host=args.host, port=args.port, log_level="info")
        elif mode == "pdf":
            from dataflow.webui import kbclean_webui
            kbclean_webui.create_ui().launch()
        else:
            parser.error(f"Unknown ui_mode {mode!r}")


if __name__ == "__main__":
    main()